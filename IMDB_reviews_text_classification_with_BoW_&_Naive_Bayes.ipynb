{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB reviews text classification with BoW & Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6h_G9DupxS1Y",
        "BGMIdzdFbaqP",
        "f8FR_xndbg2h",
        "A4Srx1hJcIZB",
        "SqtwznRb4kLv",
        "MvKehmqv3vP-",
        "T8Lk-Vqc33N8",
        "ZQCAqZWv5as3",
        "9qQUV0v2k0hU",
        "Zht_uOjSjfUW",
        "1wvLygkWjuvT",
        "jQ1iuaqmuzGi",
        "fkD13_OIvkEl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3sxmzlN0CIq"
      },
      "source": [
        "## Findings -:-\n",
        "\n",
        "In this use case I tried to investigate how Naive Bayes performs if we use CountVectorizer as feature extractor. \n",
        "\n",
        "I also wanted to investigate how does lemmatization and Stemming can affect the model performace given the same text cleaning was used for both (cleaning html tags, stopwords, punctuations)\n",
        "\n",
        "- Total training sentences,labels = 25000\n",
        "\n",
        "- Total Validation sentences,labels = 10000\n",
        "\n",
        "Below are the observations listed - \n",
        "\n",
        "1. Comparing Performance of Naive Bayes model\n",
        "\n",
        "    Both lemmatizer(WordNetLemmatizer) and stemmer(PorterStemmer) performed quiet good with ~ 82% accuracy and f1-score around 83% & 81% for class 0 and 1 respectively. Since there were no significant class imbalance in both training and validation set, so the accuarcy_score would be a reliable metric in this case.\n",
        "\n",
        "2. A little overfitting was noticed (accuracy on training sentences was ~88% and this is expected as I've used maximum_features for the Vecorizer to be 20000 only, so it might be the case that many of the words in the validation data may not be present in the training data during creating BoW. Default ngram_range = (1,1) was taken.\n",
        "\n",
        "3. When used nltk.word_tokenize( ) on the stemmed sentences, it was seen that no. of tokens generated a little less than when used lemmatized senetences.\n",
        "\n",
        "  - Total no. of tokens generated from Lemmatized sentences = 3041167\n",
        "  - Total no. of tokens generated from Stemmed sentences = 3041155\n",
        "  - % of unique tokens generated from Lemmatized sentences = 4.07%\n",
        "  - % of unique tokens generated from Stemmed sentences = 3.44%\n",
        "\n",
        "2. On using maximum_features = None crashes runtime Google Colab multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h_G9DupxS1Y"
      },
      "source": [
        "## Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IfBP62aFAG1",
        "outputId": "25bf3cc5-0e91-495a-85eb-87e3cabef1a1"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
        "from sklearn.metrics import classification_report,roc_auc_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMIdzdFbaqP"
      },
      "source": [
        "## Create Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAsmR7UPGEdq"
      },
      "source": [
        "# Training set\n",
        "\n",
        "ds = tfds.load('imdb_reviews', split='train', as_supervised=True)\n",
        "#ds = ds.take(1)\n",
        "\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "for text, label in tfds.as_numpy(ds):\n",
        "  #print(text.decode('UTF-8'), type(label), label)\n",
        "  training_sentences.append(text.decode('UTF-8')) #to convert bytes into strings\n",
        "  training_labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i28GMftUTNH",
        "outputId": "5c2cda21-a80a-402d-e7d4-e2c0310bcc21"
      },
      "source": [
        "train_df = pd.DataFrame(np.column_stack((training_sentences,training_labels)), \n",
        "                        columns = ['Sentences', 'Labels'])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This was an absolutely terrible movie. Don't b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have been known to fall asleep during films,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mann photographs the Alberta Rocky Mountains i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is the kind of film for a snowy Sunday af...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As others have mentioned, all the women that g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences Labels\n",
              "0  This was an absolutely terrible movie. Don't b...      0\n",
              "1  I have been known to fall asleep during films,...      0\n",
              "2  Mann photographs the Alberta Rocky Mountains i...      0\n",
              "3  This is the kind of film for a snowy Sunday af...      1\n",
              "4  As others have mentioned, all the women that g...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sADnF6zSrA-",
        "outputId": "bddfb88c-f47f-4780-c424-69b96b7cb2b9"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8FR_xndbg2h"
      },
      "source": [
        "## Create Validation Data/Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SCKsRf-HHdS",
        "outputId": "2b456e71-a1bc-4bfb-a998-7e496515ea74"
      },
      "source": [
        "# Validation set\n",
        "ds = tfds.load('imdb_reviews', split='test', as_supervised=True)\n",
        "ds = ds.take(10000)\n",
        "\n",
        "valid_sentences = []\n",
        "valid_labels = []\n",
        "\n",
        "for text, label in tfds.as_numpy(ds):\n",
        "  #print(text.decode('UTF-8'), type(label), label)\n",
        "  valid_sentences.append(text.decode('UTF-8')) #to convert bytes into strings\n",
        "  valid_labels.append(label)\n",
        "\n",
        "valid_df = pd.DataFrame(np.column_stack((valid_sentences,valid_labels)), \n",
        "                        columns = ['Sentences', 'Labels'])\n",
        "valid_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There are films that make careers. For George ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A blackly comic tale of a down-trodden priest,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scary Movie 1-4, Epic Movie, Date Movie, Meet ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Poor Shirley MacLaine tries hard to lend some ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As a former Erasmus student I enjoyed this fil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences Labels\n",
              "0  There are films that make careers. For George ...      1\n",
              "1  A blackly comic tale of a down-trodden priest,...      1\n",
              "2  Scary Movie 1-4, Epic Movie, Date Movie, Meet ...      0\n",
              "3  Poor Shirley MacLaine tries hard to lend some ...      0\n",
              "4  As a former Erasmus student I enjoyed this fil...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJYu2PhgbVt4",
        "outputId": "5e27adf2-e9b1-4944-879e-1c6565874b18"
      },
      "source": [
        "valid_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Srx1hJcIZB"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqtwznRb4kLv"
      },
      "source": [
        "### Functions for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1m8OcEbXZM"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        " text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        " text = text.lower() # lowercase text\n",
        " text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        " text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        " text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        " return text\n",
        "\n",
        "def stemming(sentence):\n",
        " \n",
        " stemmer = PorterStemmer()\n",
        " stemmed = [stemmer.stem(word) for word in sentence.split()]\n",
        " #stemSentence = stemSentence.strip()\n",
        " return ' '.join(stemmed)\n",
        "\n",
        "\n",
        "# Lemmatizer function\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "lmtzr = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(sentence):\n",
        "  lemmatized_sentence = []\n",
        "  tokens = word_tokenize(sentence)\n",
        "  for token, tag in pos_tag(tokens):  # returns token and corresponding tag from list of tokens\n",
        "    lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "    #print(token, \"=>\", lemma)\n",
        "    lemmatized_sentence.append(lemma)\n",
        "  return ' '.join(lemmatized_sentence)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvKehmqv3vP-"
      },
      "source": [
        "### Proprocessing Training Senetences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2l5J-7C2kgD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "tokens = word_tokenize(train_df.loc[0,'Sentences'])\n",
        "\n",
        "print(pos_tag(tokens))\n",
        "\n",
        "[('absolutely', 'RB'), ('terrible', 'JJ'), ('movie', 'NN'), ('dont', 'NN'), ('lured', 'VBD'), ('christopher', 'JJR'), ('walken', 'NN'), ('michael', 'NN'), ('ironside', 'VBP'), ('great', 'JJ'), ('actors', 'NNS'), ('must', 'MD'), ('simply', 'RB'), ('worst', 'VB'), ('role', 'NN'), ('history', 'NN'), ('even', 'RB'), ('great', 'JJ'), ('acting', 'VBG'), ('could', 'MD'), ('redeem', 'VB'), ('movies', 'NNS'), ('ridiculous', 'JJ'), ('storyline', 'JJ'), ('movie', 'NN'), ('early', 'JJ'), ('nineties', 'NNS'), ('us', 'PRP'), ('propaganda', 'VBP'), ('piece', 'JJ'), ('pathetic', 'JJ'), ('scenes', 'NNS'), ('columbian', 'JJ'), ('rebels', 'NNS'), ('making', 'VBG'), ('cases', 'NNS'), ('revolutions', 'NNS'), ('maria', 'VBP'), ('conchita', 'JJ'), ('alonso', 'NN'), ('appeared', 'VBD'), ('phony', 'JJ'), ('pseudolove', 'NN'), ('affair', 'NN'), ('walken', 'IN'), ('nothing', 'NN'), ('pathetic', 'JJ'), ('emotional', 'JJ'), ('plug', 'NN'), ('movie', 'NN'), ('devoid', 'JJ'), ('real', 'JJ'), ('meaning', 'NN'), ('disappointed', 'JJ'), ('movies', 'NNS'), ('like', 'IN'), ('ruining', 'VBG'), ('actors', 'NNS'), ('like', 'IN'), ('christopher', 'NN'), ('walkens', 'NNS'), ('good', 'JJ'), ('name', 'NN'), ('could', 'MD'), ('barely', 'RB'), ('sit', 'VB')]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQoYN06ElMdK",
        "outputId": "89bcbdf0-8e03-4f9c-e6d6-a3af11b05289"
      },
      "source": [
        "train_df['Sentences'] = train_df['Sentences'].apply(lambda x : clean_text(x))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>absolutely terrible movie dont lured christoph...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>known fall asleep films usually due combinatio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mann photographs alberta rocky mountains super...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kind film snowy sunday afternoon rest world go...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>others mentioned women go nude film mostly abs...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences Labels\n",
              "0  absolutely terrible movie dont lured christoph...      0\n",
              "1  known fall asleep films usually due combinatio...      0\n",
              "2  mann photographs alberta rocky mountains super...      0\n",
              "3  kind film snowy sunday afternoon rest world go...      1\n",
              "4  others mentioned women go nude film mostly abs...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCeRThsGmWGH",
        "outputId": "b3a6c832-ac57-4aa9-977e-33d73c64595b"
      },
      "source": [
        "train_df['Stemmed']  = train_df['Sentences'].apply(lambda x : stemming(x))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>absolutely terrible movie dont lured christoph...</td>\n",
              "      <td>0</td>\n",
              "      <td>absolut terribl movi dont lure christoph walke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>known fall asleep films usually due combinatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>known fall asleep film usual due combin thing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mann photographs alberta rocky mountains super...</td>\n",
              "      <td>0</td>\n",
              "      <td>mann photograph alberta rocki mountain superb ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kind film snowy sunday afternoon rest world go...</td>\n",
              "      <td>1</td>\n",
              "      <td>kind film snowi sunday afternoon rest world go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>others mentioned women go nude film mostly abs...</td>\n",
              "      <td>1</td>\n",
              "      <td>other mention women go nude film mostli absolu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences  ...                                            Stemmed\n",
              "0  absolutely terrible movie dont lured christoph...  ...  absolut terribl movi dont lure christoph walke...\n",
              "1  known fall asleep films usually due combinatio...  ...  known fall asleep film usual due combin thing ...\n",
              "2  mann photographs alberta rocky mountains super...  ...  mann photograph alberta rocki mountain superb ...\n",
              "3  kind film snowy sunday afternoon rest world go...  ...  kind film snowi sunday afternoon rest world go...\n",
              "4  others mentioned women go nude film mostly abs...  ...  other mention women go nude film mostli absolu...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9Cc4zyQnvOm",
        "outputId": "cb0d1ede-beaf-425c-d55f-e4413f238c8b"
      },
      "source": [
        "train_df['Lemmatized'] =  train_df['Sentences'].apply(lambda x : lemmatize(x))\n",
        "train_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Stemmed</th>\n",
              "      <th>Lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>absolutely terrible movie dont lured christoph...</td>\n",
              "      <td>0</td>\n",
              "      <td>absolut terribl movi dont lure christoph walke...</td>\n",
              "      <td>absolutely terrible movie dont lure christophe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>known fall asleep films usually due combinatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>known fall asleep film usual due combin thing ...</td>\n",
              "      <td>know fall asleep film usually due combination ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences  ...                                         Lemmatized\n",
              "0  absolutely terrible movie dont lured christoph...  ...  absolutely terrible movie dont lure christophe...\n",
              "1  known fall asleep films usually due combinatio...  ...  know fall asleep film usually due combination ...\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUOmYSUpzsdP",
        "outputId": "c1e1dcbc-6f82-4293-a7f6-0bf1dfb65320"
      },
      "source": [
        "train_df['Lemmatized'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'absolutely terrible movie dont lure christopher walken michael ironside great actor must simply worst role history even great act could redeem movie ridiculous storyline movie early ninety u propaganda piece pathetic scene columbian rebel make case revolution maria conchita alonso appear phony pseudolove affair walken nothing pathetic emotional plug movie devoid real meaning disappointed movie like ruin actor like christopher walkens good name could barely sit'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Lk-Vqc33N8"
      },
      "source": [
        "### Proprocessing Validation Senetences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjST4sgT3QZr",
        "outputId": "64ce05c6-d1a1-40d7-9e49-a4180a053f2f"
      },
      "source": [
        "valid_df['Sentences'] = valid_df['Sentences'].apply(lambda x : clean_text(x)) # clean all punctuations, lowercase, remove stopwords\n",
        "valid_df['Stemmed']  = valid_df['Sentences'].apply(lambda x : stemming(x)) # Stemming\n",
        "valid_df['Lemmatized'] =  valid_df['Sentences'].apply(lambda x : lemmatize(x)) # Lemmatization\n",
        "valid_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Stemmed</th>\n",
              "      <th>Lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>films make careers george romero night living ...</td>\n",
              "      <td>1</td>\n",
              "      <td>film make career georg romero night live dead ...</td>\n",
              "      <td>film make career george romero night live dead...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blackly comic tale downtrodden priest nazarin ...</td>\n",
              "      <td>1</td>\n",
              "      <td>blackli comic tale downtrodden priest nazarin ...</td>\n",
              "      <td>blackly comic tale downtrodden priest nazarin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>scary movie 14 epic movie date movie meet spar...</td>\n",
              "      <td>0</td>\n",
              "      <td>scari movi 14 epic movi date movi meet spartan...</td>\n",
              "      <td>scary movie 14 epic movie date movie meet spar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>poor shirley maclaine tries hard lend gravitas...</td>\n",
              "      <td>0</td>\n",
              "      <td>poor shirley maclain tri hard lend gravita maw...</td>\n",
              "      <td>poor shirley maclaine try hard lend gravitas m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>former erasmus student enjoyed film much reali...</td>\n",
              "      <td>1</td>\n",
              "      <td>former erasmu student enjoy film much realist ...</td>\n",
              "      <td>former erasmus student enjoy film much realist...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentences  ...                                         Lemmatized\n",
              "0  films make careers george romero night living ...  ...  film make career george romero night live dead...\n",
              "1  blackly comic tale downtrodden priest nazarin ...  ...  blackly comic tale downtrodden priest nazarin ...\n",
              "2  scary movie 14 epic movie date movie meet spar...  ...  scary movie 14 epic movie date movie meet spar...\n",
              "3  poor shirley maclaine tries hard lend gravitas...  ...  poor shirley maclaine try hard lend gravitas m...\n",
              "4  former erasmus student enjoyed film much reali...  ...  former erasmus student enjoy film much realist...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQCAqZWv5as3"
      },
      "source": [
        "## Let's checkout the distributions of target in the training set & dev set and decide the performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ56nEBv5Y2A",
        "outputId": "ade8e5d6-611e-449e-f77b-90ab525075c5"
      },
      "source": [
        "sns.countplot(train_df['Labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f996243fb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC4FFxUS6nAR",
        "outputId": "d7e26c60-41d3-406c-a5b6-162bc2f89d44"
      },
      "source": [
        "train_df['Labels'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12500\n",
              "1    12500\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIrx0-lq51Iw",
        "outputId": "73d6d5d4-0cd3-43d5-b309-9e0a64d4d68d"
      },
      "source": [
        "sns.countplot(valid_df['Labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9951754190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laR5d74M6hme",
        "outputId": "52f0f634-04b7-4a06-f7e7-7db7b5ba0e13"
      },
      "source": [
        "valid_df['Labels'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5061\n",
              "1    4939\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnEGvF6e6A4B"
      },
      "source": [
        "So, from the above plots we see both labels in training data and test data are quiet balanced. So we can use accuracy score as metrics in this case. But for now, I'll try to use f1-score as performance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpx8tR8H7Bja"
      },
      "source": [
        "## Let's create a BOW model with sklearn's CountVectorizer \n",
        "_We'll try with both lemmatized and stemmed texts to check is there difference in the performace of the model_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qQUV0v2k0hU"
      },
      "source": [
        "### Analysis of tokens in Lemmatized and Stemmed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woovEXNm8cDA",
        "outputId": "ef92f69b-e452-4d46-f38d-42d02dd46d2e"
      },
      "source": [
        "# Lemmatized sentences\n",
        "lemma_corpus = [word_tokenize(sentence) for sentence in train_df['Lemmatized']]\n",
        "lemma_corpus = list(itertools.chain.from_iterable(lemma_corpus)) # to jooin the list of lists\n",
        "print('Total no. of sentences tokenized:', len(train_df['Lemmatized']))\n",
        "print('Unique tokens: ',len(set(lemma_corpus)))\n",
        "print('Total no. of tokens: ',len(lemma_corpus))\n",
        "print('% of unique tokens: ','{:.3f}'.format(len(set(lemma_corpus))/len(lemma_corpus)*100),' %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no. of sentences tokenized: 25000\n",
            "Unique tokens:  123767\n",
            "Total no. of tokens:  3041167\n",
            "% of unique tokens:  4.070  %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rUSViqRSAS2",
        "outputId": "b9cb8fd3-7b81-4985-dc23-45b5814fea6a"
      },
      "source": [
        "lemma_corpus[:7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['absolutely', 'terrible', 'movie', 'dont', 'lure', 'christopher', 'walken']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZNatihFSECf",
        "outputId": "ee7c3810-c3be-4ab9-999d-118ee616de2b"
      },
      "source": [
        "# Stemmed Sentences\n",
        "stem_corpus = [word_tokenize(sentence) for sentence in train_df['Stemmed']]\n",
        "stem_corpus = list(itertools.chain.from_iterable(stem_corpus)) # to jooin the list of lists\n",
        "print('Total no. of sentences tokenized:', len(train_df['Stemmed']))\n",
        "print('Unique tokens: ',len(set(stem_corpus)))\n",
        "print('Total no. of tokens: ',len(stem_corpus))\n",
        "print('% of unique tokens: ','{:.3f}'.format(len(set(stem_corpus))/len(stem_corpus)*100),' %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no. of sentences tokenized: 25000\n",
            "Unique tokens:  104523\n",
            "Total no. of tokens:  3041155\n",
            "% of unique tokens:  3.437  %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OOA93WaS5_i",
        "outputId": "9ab09be6-823f-4b93-a223-865234a53820"
      },
      "source": [
        "stem_corpus[:7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['absolut', 'terribl', 'movi', 'dont', 'lure', 'christoph', 'walken']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UG7lt7OT6Wq"
      },
      "source": [
        "From the above comparison we see that Lemmatization creates more no.of unique tokens than Stemming in this use case. If we see the total count of tokens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GINPthqwYlxE",
        "outputId": "e4db4ad9-d23b-4792-e7ef-8fc7f89700b8"
      },
      "source": [
        "all_training_sentences_lemma = [sentence for sentence in train_df['Lemmatized']]\n",
        "all_training_sentences_stem = [sentence for sentence in train_df['Stemmed']]\n",
        "print(len(all_training_sentences_lemma))\n",
        "print(len(all_training_sentences_stem))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe7WVJkrhJwq",
        "outputId": "e9ce1ce1-cd9f-441c-d11b-721058724df9"
      },
      "source": [
        "all_validation_sentences_lemma = [sentence for sentence in valid_df['Lemmatized']]\n",
        "all_validation_sentences_stem = [sentence for sentence in valid_df['Stemmed']]\n",
        "print(len(all_validation_sentences_lemma))\n",
        "print(len(all_validation_sentences_stem))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOmNoJhWim9O"
      },
      "source": [
        "### Let's create the BOWs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zht_uOjSjfUW"
      },
      "source": [
        "#### Lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEk5p1388yT7",
        "outputId": "a5f6e750-946e-4b9c-9a1d-d79b97b25c57"
      },
      "source": [
        "cvector_lemma = CountVectorizer(max_features = 20000)\n",
        "lemma_train_bow = cvector_lemma.fit_transform(all_training_sentences_lemma).toarray()\n",
        "lemma_train_bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRcsmPy084fT",
        "outputId": "0d0eef57-29ea-4b35-dd6c-d76cf6e2e961"
      },
      "source": [
        "lemma_test_bow = cvector_lemma.transform(all_validation_sentences_lemma)\n",
        "lemma_test_bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wvLygkWjuvT"
      },
      "source": [
        "#### Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E40rqJAkHHn2",
        "outputId": "1b87b6f2-3f63-4978-bc9f-17beba9f9c89"
      },
      "source": [
        "cvector_stem = CountVectorizer(max_features = 20000)\n",
        "stem_train_bow = cvector_stem.fit_transform(all_training_sentences_stem).toarray()\n",
        "stem_train_bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Fa8ZH5-KJE",
        "outputId": "92bda6bd-0bad-4d4b-e3af-f715b1bbfb8e"
      },
      "source": [
        "stem_test_bow = cvector_stem.transform(all_validation_sentences_stem).toarray()\n",
        "stem_test_bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEyKE17PoP5J"
      },
      "source": [
        "## Training a Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ1iuaqmuzGi"
      },
      "source": [
        "### Naive Bayes on Lemmatized Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9O9FKbkHPbG"
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(lemma_train_bow,np.array(training_labels))\n",
        "predicted_labels = classifier.predict(lemma_test_bow)\n",
        "predicted_labels_train = classifier.predict(lemma_train_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cao_n2hWsSbc",
        "outputId": "34b281db-fa9a-48f6-d918-a3e0bf740454"
      },
      "source": [
        "print(\"Validation Set\")\n",
        "print(classification_report(np.array(valid_labels),predicted_labels))\n",
        "print(\"Training Set\")\n",
        "print(classification_report(np.array(training_labels),predicted_labels_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.87      0.83      5061\n",
            "           1       0.85      0.76      0.81      4939\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.82      0.82     10000\n",
            "\n",
            "Training Set\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.88     12500\n",
            "           1       0.90      0.86      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkD13_OIvkEl"
      },
      "source": [
        "### Naive Bayes on Stemmed Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CIXFhm0vjNH"
      },
      "source": [
        "classifier2 = MultinomialNB()\n",
        "classifier2.fit(stem_train_bow, np.array(training_labels))\n",
        "predicted_labels_stem = classifier2.predict(stem_test_bow)\n",
        "predicted_labels_stem_train = classifier2.predict(stem_train_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3wFLvBcwMmD",
        "outputId": "3cbfce17-bd09-4038-bc4a-709a3ef867e1"
      },
      "source": [
        "print(\"Validation Data\")\n",
        "print(classification_report(np.array(valid_labels),predicted_labels_stem))\n",
        "print(\"Training Data\")\n",
        "print(classification_report(np.array(training_labels),predicted_labels_stem_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.87      0.83      5061\n",
            "           1       0.85      0.76      0.80      4939\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.82      0.81      0.81     10000\n",
            "weighted avg       0.82      0.81      0.81     10000\n",
            "\n",
            "Training Data\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.88     12500\n",
            "           1       0.90      0.86      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}